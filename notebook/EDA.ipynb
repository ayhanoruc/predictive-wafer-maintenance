{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM DEFINITION\n",
    "  \n",
    "- In semiconductor manufacturing, the production of integrated circuits involves multiple complex processes, including wafer fabrication. Wafers are thin, round substrates made of semiconductor materials, and they undergo various manufacturing steps to create microchips. The quality of these wafers is critical for ensuring the reliability and performance of the final IC's.\n",
    "\n",
    "- The manufacturing process of semiconducter wafers is susceptible to various defects and faults that can compromise the quality and yield of IC's. These faults can result from contamination, equipment malfunctions or process variations. Detecting and classifying these faults early in the manufacturing process is essential to minimze waste and ensure product quality.\n",
    "\n",
    "## GOAL\n",
    "- The goal of this experiment notebook is to findout a succesful / generalized predictive classifier model that can accurately classify semiconducter wafers as either \"Good\" or \"Bad\" based on sensor data collected during the manufacturing process. \n",
    "- According to the domain knowledge of expertizes and regarding business priorities and potential consequences of false predicted cases, the cost function is calculated to be 10.FN + 1.FP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x]head, info, description\n",
    "\n",
    "[x]quick profiling reports\n",
    "\n",
    "[x] missing values analysis\n",
    "\n",
    "[x]duplicates\n",
    "\n",
    "[x]outliers\n",
    "\n",
    "[x]feature distributions\n",
    "\n",
    "[x]imbalance check\n",
    "\n",
    "[x]correlation analysis\n",
    "\n",
    "[x]notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "- all meaningful input features are float64 dtype\n",
    "- wafer id can be removed\n",
    "- no duplicated rows\n",
    "- raw data is highly unscaled\n",
    "- lots of outliers, even when i set iqr_threshold to 5% and coefficient to 5, there were still lots of outliers\n",
    "- as an alternative, i peformed multivariate outlier detection using LOF, the elbow method suggested the threshold score to be <-2\n",
    "- there is no duplicated rows, but are a lot of (more than 100) duplicated columns, may be these are all zero columns.\n",
    "- there are 112 columns of constant value \"0\" \n",
    "- best_fit_distribution types are found , re-assess after data transformation\n",
    "- highly imbalanced dataset, handle imbalance with stratified kfold when splitting as train-test dataset\n",
    "- correlation study is performed with filter condition of abs(corr)>95 or abs(cor)<100 and if the correlated column count > threshold=5  , some columns meeting these conditions are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayhan\\Desktop\\predictive-wafer-maintenance\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import dtale\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats \n",
    "import scipy \n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score, roc_auc_score, roc_curve, precision_score,recall_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from imblearn.combine import SMOTETomek # hybrid technique\n",
    "\n",
    "from sklearn.base import BaseEstimator , TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import optuna \n",
    "import dill\n",
    "\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    max-height: 800px; /* Adjust the width as needed */\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    max-height: 800px; /* Adjust the width as needed */\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_dir = \"../valid_feature_store/valid_training_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_original_data()->pd.DataFrame:\n",
    "    csv_file_list = os.listdir(valid_dataset_dir)\n",
    "    df_merged = pd.DataFrame()\n",
    "    for file in csv_file_list:\n",
    "        file_path = os.path.join(valid_dataset_dir,file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_merged = pd.concat(objs=[df_merged,df]) # merged around axis=0\n",
    "        df_merged.drop(columns=[\"Wafer\"],inplace=True)\n",
    "        filt = df_merged[\"Good/Bad\"]==1\n",
    "        df_merged[\"Good/Bad\"] = np.where(filt,1,0)\n",
    "\n",
    "    return df_merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = restore_original_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.describe().T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= dtale.show(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Missing Value Analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(dataframe:pd.DataFrame,is_return= False):\n",
    "    na_cols = [col for col in dataframe.columns if dataframe[col].isna().sum()>0]\n",
    "    na_data = dataframe[na_cols].isna().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[na_cols].isna().sum()/dataframe.shape[0]*100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat(objs=[na_data,np.round(ratio,2)],axis=1,keys= [\"#missing\",\"ratio\"])\n",
    "    print(missing_df,end=\"\\n\")\n",
    "    if is_return:\n",
    "        return (na_cols,missing_df)\n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_table = missing_values_table(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_missed_table = missing_table.query(\"ratio>60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_missed_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ->missing value analysis w/ target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_vs_target(dataframe:pd.DataFrame,target:str,na_cols:list):\n",
    "    i=0\n",
    "    temp_df = dataframe.copy()\n",
    "    for col in na_cols:\n",
    "        temp_df[col + \"_NA_FLAG\"] = np.where(temp_df[col].isna(),1,0)\n",
    "    \n",
    "    na_flags= [col for col in temp_df.columns if \"_NA_FLAG\" in col ]\n",
    "    \n",
    "\n",
    "    \n",
    "    for col in na_flags:\n",
    "        table= pd.DataFrame({\n",
    "            \"Target_Mean\": temp_df.groupby(col)[target].mean(),\n",
    "            \"count\":temp_df.groupby(col)[target].count()\n",
    "        })\n",
    "\n",
    "        if (abs(table.iloc[1,0])> 0.30) and  (abs(table.iloc[1,0])< 0.70) :\n",
    "            i+=1\n",
    "            print(table,end=\"\\n\\n\")\n",
    "\n",
    "        \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = missing_values_table(df_merged,is_return=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vs_target(df_merged,\"Good/Bad\",na_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_row_analysis(dataframe:pd.DataFrame):\n",
    "    table= pd.DataFrame({\n",
    "        \"#missing\":dataframe.isna().sum(axis=1).sort_values(ascending=False),\n",
    "        \"ratio\": dataframe.isna().sum(axis=1).sort_values(ascending=False)/ dataframe.shape[0]*100\n",
    "    })\n",
    "    return table \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_row_analysis(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(16, 16))\n",
    "#print(axs)\n",
    "\n",
    "for i, col in enumerate(df_merged.columns[:25]):\n",
    "    row = i // 5  \n",
    "    col_num = i % 5  \n",
    "    #print(row,col_num)\n",
    "    sns.boxplot(y=df_merged[col], ax=axs[row, col_num])\n",
    "    axs[row, col_num].set_title(\"Values\")\n",
    "    axs[row, col_num].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_threshold(dataframe:pd.DataFrame,col_name:str,threshold,coeff):\n",
    "    q1= dataframe[col_name].quantile(threshold)\n",
    "    q3 = dataframe[col_name].quantile(1-threshold)\n",
    "    iqr = q3-q1 \n",
    "    upper = q3 + coeff*iqr \n",
    "    lower = q1 - coeff*iqr\n",
    "    return lower,upper \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(dataframe:pd.DataFrame,col_name,index=True,threshold:float=0.05,coeff=1.5):\n",
    "    lower,upper = iqr_threshold(dataframe,col_name,threshold,coeff)\n",
    "    filt = (dataframe[col_name] > upper) | (dataframe[col_name] < lower)\n",
    "    if index:\n",
    "        return (dataframe[filt].index, dataframe.loc[dataframe[filt].index,col_name])\n",
    "    else: \n",
    "        return dataframe[filt]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_merged.columns:\n",
    "    \n",
    "    indices, outliers = detect_outliers(df_merged,col,coeff=5)\n",
    "    if len(indices)>3:\n",
    "        print(col,\"mean:\",df_merged[col].mean())\n",
    "        print(indices)\n",
    "        print(outliers)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA- OUTLIER DETECTION : LOCAL OUTLIER FACTOR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=100, contamination=0.1)\n",
    "X = df_merged.fillna(0)\n",
    "norm_X = (X-X.mean())/X.std()\n",
    "y_pred = clf.fit_predict(X)\n",
    "scores = sorted(clf.negative_outlier_factor_)\n",
    "scores_2= clf.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores)\n",
    "scores.plot( xlim = [0,100],ylim = [-10,0],style='.-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[scores[0]<-2] # then we need to access the indices of these data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2 = pd.DataFrame(scores_2)\n",
    "scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outliers = scores_2[scores_2[0]<-2] # then we need to access the indices of these data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices = outliers.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in outlier_indices:\n",
    "    \n",
    "    print(df_merged.iloc[index][\"Good/Bad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUPLICATED COLUMNS\n",
    "duplicated_df = df_merged.T[df_merged.T.duplicated()]\n",
    "duplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.T[df_merged.T.duplicated()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUPLICATED ROWS:\n",
    "df_merged.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = df_merged[col]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_check = [col  for col in df_merged.columns if (df_merged[col].fillna(0)==0.0).sum()==df_merged.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zero_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_types = [stats.norm, stats.expon, stats.gamma, stats.lognorm, stats.pareto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(dataframe:pd.DataFrame, col_name):\n",
    "    best_fit = None \n",
    "    best_p_value = np.inf \n",
    "\n",
    "    for dist in dist_types:\n",
    "        params = dist.fit(dataframe[col_name].fillna(0.00001))\n",
    "        _, p_value = stats.kstest(dataframe[col_name].fillna(0.00001), dist.cdf, args=params)\n",
    "        if p_value < best_p_value:\n",
    "            best_fit = dist \n",
    "            best_p_value = p_value\n",
    "\n",
    "    print(f\"{col_name} -> Best-fit distribution: {best_fit.name}\")\n",
    "    return best_fit.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_list = []\n",
    "for col in df_merged.columns[:-1]:\n",
    "    best_fit = get_distribution(df_merged,col)\n",
    "    best_fit_list.append(best_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fits = pd.Series(best_fit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fits.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Target Imbalance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"Good/Bad\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix =  df_merged.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (abs(corr_matrix)>0.95) & (abs(corr_matrix)<1.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "corr_counts = filt.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_counts[corr_counts>2].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns[corr_counts>threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA TRANSFORMATION & FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES (COPY)\n",
    "- all meaningful input features are float64 dtype\n",
    "- wafer id can be removed\n",
    "- raw data is highly unscaled\n",
    "- lots of outliers, even when i set iqr_threshold to 5% and coefficient to 5, there were still lots of outliers\n",
    "- as an alternative, i peformed multivariate outlier detection using LOF, the elbow method suggested the threshold score to be <-2\n",
    "- there is no duplicated rows, but are a lot of (more than 100) duplicated columns, may be these are all zero columns.\n",
    "- there are 112 columns of constant value \"0\" \n",
    "- best_fit_distribution types are found , re-assess after data transformation\n",
    "- highly imbalanced dataset, handle imbalance with stratified kfold when splitting as train-test dataset\n",
    "- correlation study is performed with filter condition of abs(corr)>95 or abs(cor)<100 and if the correlated column count > threshold=5  , some columns meeting these conditions are detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA TRANFORMATION TASK LIST\n",
    "##### (!) turn all the work into modular functions / classes\n",
    "\n",
    "[x] - handle unwanted data\n",
    "\n",
    "[x] - drop zero std columns\n",
    "\n",
    "[x] - drop highly correlated columns\n",
    "\n",
    "[x] - drop duplicated columns/rows\n",
    "\n",
    "[x]  - handle missing values\n",
    "\n",
    "[x] - handle outliers\n",
    "\n",
    "[] - scale the dataframe\n",
    "\n",
    "     - provide different scaling options\n",
    "     \n",
    "     - check distributions afterwards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> DATA TRANSFORMATION: ZERO STD CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_check = [col  for col in df_merged.columns if (df_merged[col].fillna(0)==0.0).sum()==df_merged.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zero_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_merged.std() == 0 ).sum() # 112 of them are all zero columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_zero_std(dataframe:pd.DataFrame)->pd.DataFrame:\n",
    "    zero_std_cols = dataframe.columns[dataframe.std()==0]\n",
    "    dataframe2 = dataframe.drop(columns=zero_std_cols)\n",
    "    return dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero_std_dropped = drop_zero_std(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> DATA TRANSFORMATION: Drop highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_merged.corr(method=\"pearson\")\n",
    "filt = (abs(corr_matrix)>0.95) & (abs(corr_matrix)<1.00) # threshold 95% \n",
    "threshold = 5\n",
    "corr_counts = filt.sum(axis=1)\n",
    "corr_counts[corr_counts>2].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_highly_correlated_columns(dataframe:pd.DataFrame,corr_threshold=0.95,count_threshold=3):\n",
    "\n",
    "    corr_matrix= dataframe.corr(method=\"pearson\")\n",
    "    filt = (abs(corr_matrix)>corr_threshold) & (abs(corr_matrix)<1.00)\n",
    "    corr_counts = filt.sum(axis=1)\n",
    "    #highly_correlated = corr_counts[cor_counts > count_threshold].sort_values(ascending=False)\n",
    "    highly_correlated_cols = dataframe.columns[corr_counts>count_threshold]\n",
    "\n",
    "    return dataframe.drop(columns=highly_correlated_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_corr = drop_highly_correlated_columns(dataframe=df_merged,count_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_corr.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> DATA TRANSFORMATION: Drop duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.T[df_merged.T.duplicated()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicated_cols(dataframe:pd.DataFrame)->pd.DataFrame:\n",
    "    duplicated_cols = dataframe.T[dataframe.T.duplicated()].index\n",
    "    return dataframe.drop(columns=duplicated_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> DATA TRANSFORMATION: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleMissingValues:\n",
    "    def __init__(self,dataframe:pd.DataFrame):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "\n",
    "    def missing_values_table(self,is_return= True):\n",
    "\n",
    "        \"\"\"\n",
    "        Generate a summary of missing values in the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - is_return (bool)\n",
    "\n",
    "        Returns:\n",
    "        - If is_return is True, returns a tuple containing a list of columns with missing values and a DataFrame summarizing the missing values.\n",
    "        - If is_return is False, returns a DataFrame summarizing the missing values.\n",
    "        \"\"\"\n",
    "\n",
    "        na_cols = [col for col in self.dataframe.columns if self.dataframe[col].isna().sum()>0]\n",
    "        na_data = self.dataframe[na_cols].isna().sum().sort_values(ascending=False)\n",
    "        ratio = (self.dataframe[na_cols].isna().sum()/self.dataframe.shape[0]*100).sort_values(ascending=False)\n",
    "        missing_df = pd.concat(objs=[na_data,np.round(ratio,2)],axis=1,keys= [\"#missing\",\"ratio\"])\n",
    "        print(missing_df,end=\"\\n\")\n",
    "        if is_return:\n",
    "            return (na_cols,missing_df)\n",
    "        else:\n",
    "            return missing_df\n",
    "\n",
    "\n",
    "    def missing_vs_target(self,target:str,na_cols:list, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Check if missing value rows correlate with a specified target variable.\n",
    "\n",
    "        Parameters:\n",
    "        - target (str): The name of the target variable for correlation analysis.\n",
    "        - na_cols (list): A list of columns with missing values.\n",
    "        - threshold (float): The correlation threshold. Columns with correlations greater than this threshold are considered key columns.\n",
    "\n",
    "        Returns:\n",
    "        - key_cols (list): A list of column names that show strong correlation with the target variable.\n",
    "        \"\"\"\n",
    "\n",
    "        key_cols = []\n",
    "        temp_df = self.dataframe.copy()\n",
    "        for col in na_cols:\n",
    "            temp_df[col + \"_NA_FLAG\"] = np.where(temp_df[col].isna(),1,0)\n",
    "        \n",
    "        na_flags= [col for col in temp_df.columns if \"_NA_FLAG\" in col ]\n",
    "        #print(na_flags)\n",
    "        for col in na_flags:\n",
    "            table= pd.DataFrame({\n",
    "                \"Target_Mean\": temp_df.groupby(col)[target].mean(),\n",
    "                \"count\":temp_df.groupby(col)[target].count()\n",
    "            })\n",
    "            #print(table,end=\"\\n\\n\")\n",
    "\n",
    "            if (abs(table.iloc[1,0])> threshold) :\n",
    "                key_cols.append(col.replace(\"_NA_FLAG\",\"\"))\n",
    "                print(table,end=\"\\n\\n\")\n",
    "        return key_cols\n",
    "\n",
    "\n",
    "\n",
    "    def detect_highly_missing(self,missing_table, ratio_threshold=80):\n",
    "\n",
    "        \"\"\"\n",
    "        Detect columns with a high ratio of missing values.\n",
    "\n",
    "        Parameters:\n",
    "        - missing_table (pd.DataFrame): A DataFrame containing missing values information.\n",
    "        - ratio_threshold (float): The threshold for considering a column as highly missing.\n",
    "\n",
    "        Returns:\n",
    "        - highly_missing_cols (list): A list of column names with a high ratio of missing values.\n",
    "        \"\"\"\n",
    "\n",
    "        highly_missing_cols = missing_table.query(f\"ratio > {ratio_threshold}\").index \n",
    "\n",
    "        return highly_missing_cols\n",
    "        \n",
    "\n",
    "    def handle_imputation(self,method=\"constant\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Perform constant value imputation for missing values.\n",
    "\n",
    "        Parameters:\n",
    "        - constant: The constant value to use for imputing missing values : can be mean, median, mode or any constant scalar\n",
    "\n",
    "        Returns:\n",
    "        - imputed_df (pd.DataFrame): The DataFrame with missing values replaced by the constant value.\n",
    "        \"\"\"\n",
    "\n",
    "        if method in [\"mean\",\"median\"]:\n",
    "            s_imputer = SimpleImputer(strategy=method)\n",
    "            return pd.DataFrame(s_imputer.fit_transform(self.dataframe),columns=self.dataframe.columns)\n",
    "\n",
    "        elif method==\"constant\":\n",
    "            s_imputer = SimpleImputer(strategy=method,fill_value=0)\n",
    "            return pd.DataFrame(s_imputer.fit_transform(self.dataframe),columns=self.dataframe.columns)\n",
    "\n",
    "        elif method==\"knn\":\n",
    "            knn_imputer = KNNImputer(n_neighbors=10)\n",
    "            return pd.DataFrame(knn_imputer.fit_transform(self.dataframe),columns=self.dataframe.columns)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_missing = HandleMissingValues(dataframe=df_zero_std_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols , missing_table =  handle_missing.missing_values_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_missing_cols = handle_missing.detect_highly_missing(missing_table,ratio_threshold=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = handle_missing.missing_vs_target(target=\"Good/Bad\",na_cols=highly_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_missing.knn_imputation().isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> DATA TRANSFORMATION: Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleOutliers:\n",
    "    def __init__(self,dataframe:pd.DataFrame):\n",
    "        self.dataframe = dataframe \n",
    "    \n",
    "\n",
    "    \n",
    "    def iqr_threshold(self,col_name:str,threshold,coeff):\n",
    "        q1= self.dataframe[col_name].quantile(threshold)\n",
    "        q3 = self.dataframe[col_name].quantile(1-threshold)\n",
    "        iqr = q3-q1 \n",
    "        upper = q3 + coeff*iqr \n",
    "        lower = q1 - coeff*iqr\n",
    "        return lower,upper \n",
    "\n",
    "    def detect_outliers(self,col_name,index=True,threshold:float=0.05,coeff=1.5):\n",
    "        lower,upper = self.iqr_threshold(col_name,threshold,coeff)\n",
    "        filt = (self.dataframe[col_name] > upper) | (self.dataframe[col_name] < lower)\n",
    "        if index:\n",
    "            return (self.dataframe[filt].index, self.dataframe.loc[self.dataframe[filt].index,col_name])\n",
    "        else: \n",
    "            return self.dataframe[filt]\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def iqr_approach(self,col_list,threshold:float= 0.05,coeff=3):\n",
    "        indices = []\n",
    "        values = []\n",
    "        for col in col_list:\n",
    "            index_list , value_list = self.detect_outliers(col,threshold=threshold, coeff=coeff)\n",
    "            if len(index_list)>0:\n",
    "                indices.append((col,index_list))\n",
    "        return indices\n",
    "\n",
    "    \n",
    "    def find_critical_lof(self, n_neighbors=10, contamination=0.1, threshold=0.01):\n",
    "\n",
    "        lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "        y_pred = lof.fit_predict(self.dataframe)\n",
    "        scores= lof.negative_outlier_factor_\n",
    "        sorted_scores = sorted(scores)\n",
    "\n",
    "        critical_lof = None\n",
    "\n",
    "        abs_diff_scores = np.abs(np.diff(sorted_scores))\n",
    "        percentage = (abs_diff_scores/sorted_scores[:-1])\n",
    "\n",
    "        critical_index = np.argmax(abs_diff_scores < threshold)\n",
    "        \n",
    "        if critical_index > 0: \n",
    "            critical_lof = sorted_scores[critical_index]\n",
    "        \n",
    "        else:\n",
    "            critical_lof = None\n",
    "\n",
    "\n",
    "\n",
    "        # Plot the LOF scores for the specified range of n_neighbors\n",
    "        plt.plot(range(len(scores)),sorted_scores,marker = 'o', linestyle='-')\n",
    "        plt.xlabel('data')\n",
    "        plt.ylabel('LOF Scores')\n",
    "        plt.title('LOF Score vs. data')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        return (critical_lof,pd.DataFrame(scores))\n",
    "\n",
    "\n",
    "    \"\"\"def multivariate_w_lof(self,n_neighbors=20,contamination=0.1):\n",
    "        # This should be applied to datasets with no missing values\n",
    "\n",
    "        lof = LocalOutlierFactor(n_neighbors=10)\n",
    "        y_pred = lof.fit_predict(self.dataframe)\n",
    "        scores= sorted(lof.negative_outlier_factor_)\n",
    "        scores = pd.DataFrame(scores)\n",
    "        scores.plot( xlim = [0,100],ylim = [-10,0],style='.-')\n",
    "        plt.show()\n",
    "        return scores\"\"\"\n",
    "\n",
    "    def drop_outliers(self,col,row_list):\n",
    "        temp_df = self.dataframe.copy()\n",
    "        for row in row_list:\n",
    "            temp_df.at[row,col] = float(\"nan\")\n",
    "\n",
    "        return temp_df\n",
    "    \n",
    "    def impute_outliers(self,value):\n",
    "        pass \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outliers = HandleOutliers(df_zero_std_dropped.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = handle_outliers.iqr_approach(df_zero_std_dropped.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for col,ind_list in ind:\n",
    "    counter += len(ind_list)\n",
    "\n",
    "counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_point,scores = handle_outliers.find_critical_lof(threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[scores[0]<elbow_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ind = scores[scores[0]<elbow_point].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero_std_dropped.iloc[out_ind][\"Good/Bad\"].value_counts()  # thi suggests that being identified as an outlier by LOF  does not mean that you are a fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> DATA TRANSFORMATION: Handle Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleScaling:\n",
    "    def __init__(self,dataframe:pd.DataFrame):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def standard_scaler(self):\n",
    "        ss = StandardScaler()\n",
    "        return pd.DataFrame(ss.fit_transform(self.dataframe),columns=self.dataframe.columns)\n",
    "    \n",
    "    def robust_scaler(self):\n",
    "        rs = RobustScaler()\n",
    "        return pd.DataFrame(rs.fit_transform(self.dataframe),columns=self.dataframe.columns)\n",
    "\n",
    "\n",
    "    def min_max_scaler(self,feature_range=(0,1)):\n",
    "        mms = MinMaxScaler(feature_range=feature_range)\n",
    "        return pd.DataFrame(mms.fit_transform(self.dataframe),columns=self.dataframe.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X,y):\n",
    "    smt = SMOTETomek(random_state=11,sampling_strategy=\"minority\")\n",
    "    return smt.fit_resample(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <<< NOW SCALE ALL THE COLUMNS THEN CHECK DISTRIBUTION TYPES >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SELECTION/TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(true,predicted):\n",
    "    acc = accuracy_score(true,predicted)\n",
    "    f1 = f1_score(true,predicted)\n",
    "    precision = precision_score(true,predicted)\n",
    "    recall = recall_score(true,predicted)\n",
    "    roc_auc = roc_auc_score(true,predicted)\n",
    "    \n",
    "    return acc, f1, precision, recall , roc_auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(true,pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(true,pred).ravel()\n",
    "    cost = 10*fn + 1*fp \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X,y,models:dict)->pd.DataFrame:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=11)\n",
    "    \n",
    "    models_list = []\n",
    "    accuracy_list = []\n",
    "    cost_list = []\n",
    "\n",
    "    for model_name, model_obj in models.items():\n",
    "        model_obj.fit(X_train,y_train)\n",
    "\n",
    "        y_train_pred = model_obj.predict(X_train)\n",
    "        y_test_pred = model_obj.predict(X_test)\n",
    "\n",
    "        # model performance on training dataset\n",
    "        train_acc, train_f1, train_precision, train_recall , train_roc_auc = evaluate_clf(y_train,y_train_pred) \n",
    "        train_cost = total_cost(y_train, y_train_pred)\n",
    "\n",
    "        # model performance on testing dataset\n",
    "        test_acc, test_f1, test_precision, test_recall , test_roc_auc = evaluate_clf(y_test,y_test_pred) \n",
    "        test_cost = total_cost(y_test, y_test_pred)\n",
    "\n",
    "        print(model_name)\n",
    "        models_list.append(model_name)  \n",
    "\n",
    "\n",
    "        print('Model performance for Training set')\n",
    "        print(\"- Accuracy: {:.4f}\".format(train_acc))\n",
    "        print('- F1 score: {:.4f}'.format(train_f1)) \n",
    "        print('- Precision: {:.4f}'.format(train_precision))\n",
    "        print('- Recall: {:.4f}'.format(train_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(train_roc_auc))\n",
    "        print(f'- COST: {train_cost}.')\n",
    "\n",
    "        print('----------------------------------')\n",
    "\n",
    "        print('Model performance for Test set')\n",
    "        print('- Accuracy: {:.4f}'.format(test_acc))\n",
    "        print('- F1 score: {:.4f}'.format(test_f1))\n",
    "        print('- Precision: {:.4f}'.format(test_precision))\n",
    "        print('- Recall: {:.4f}'.format(test_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(test_roc_auc))\n",
    "        print(f'- COST: {test_cost}.')\n",
    "        cost_list.append(test_cost)\n",
    "        print('='*35)\n",
    "        print('\\n')\n",
    "        \n",
    "    report = pd.DataFrame(list(zip(models_list,cost_list)),columns=[\"Model Name\",\"Cost\"])\n",
    "    return report \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we initialize the models we want to investigate in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after checking perfomances of various models, ensemble classifiers especially XGBClassifier outperforms and generalizes the best.\n",
    "# Furthermore, we could train a neural network also, but the results from XGBClassifier was satisfactory.\n",
    "\n",
    "models = {\n",
    "    \"XGBClassifier\" : XGBClassifier(\n",
    "        random_state=11,\n",
    "        scale_pos_weight=30,\n",
    "        reg_alpha=0.01),\n",
    " \n",
    "    #\"IsolationForest\":IsolationForest(n_estimators=100,max_samples=500, \n",
    "    #            contamination=0.1,random_state=11, verbose=0)\n",
    "    #\"SVC\": SVC(probability=True,class_weight={0: 1, 1: 50}),\n",
    "    #\"knn\":KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some common data transformations before Model Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "- all meaningful input features are float64 dtype\n",
    "- wafer id can be removed\n",
    "- no duplicated rows\n",
    "- raw data is highly unscaled\n",
    "- lots of outliers, even when i set iqr_threshold to 5% and coefficient to 5, there were still lots of outliers\n",
    "- as an alternative, i peformed multivariate outlier detection using LOF, the elbow method suggested the threshold score to be <-2\n",
    "- there is no duplicated rows, but are a lot of (more than 100) duplicated columns, may be these are all zero columns.\n",
    "- there are 112 columns of constant value \"0\" \n",
    "- best_fit_distribution types are found , re-assess after data transformation\n",
    "- highly imbalanced dataset, handle imbalance with stratified kfold when splitting as train-test dataset\n",
    "- correlation study is performed with filter condition of abs(corr)>95 or abs(cor)<100 and if the correlated column count > threshold=5  , some columns meeting these conditions are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = drop_zero_std(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = drop_duplicated_cols(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing value imputation = fillna with [constant(0, mean, median),knn imputer]\n",
    "\n",
    "outlier detection = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(dataframe):\n",
    "    X= dataframe.drop(\"Good/Bad\", axis=\"columns\")\n",
    "    y= dataframe[\"Good/Bad\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=11,stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"XGBClassifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [i*0.0005 for i in range(1,1200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr[\"Sensor_id\"] = df_merged.columns[:-1].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_score = [abs(round(df_merged[[col,\"Good/Bad\"]].corr().iloc[0,1],4)) for col in df_merged.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr[\"corr\"] = corr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.sort_values(by=\"corr\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_corr_cols = df_corr.sort_values(by=\"corr\",ascending=False)[:200][\"Sensor_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_cols(dataframe:pd.DataFrame)->list:\n",
    "    df_corr = pd.DataFrame()\n",
    "    df_corr[\"Sensor_id\"] = dataframe.columns[:-1].tolist()\n",
    "    corr_score = [abs(round(dataframe[[col,\"Good/Bad\"]].corr().iloc[0,1],4)) for col in dataframe.columns[:-1]]\n",
    "    df_corr[\"corr\"] = corr_score\n",
    "    important_cols = df_corr.sort_values(by=\"corr\",ascending=False)[:200][\"Sensor_id\"].to_list()\n",
    "    return important_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(X_train,X_test,y_train,y_test, models:dict, threshold_list:list)->int:\n",
    "\n",
    "    cost_list = [] \n",
    "\n",
    "    for model_name, model_obj in models.items():\n",
    "\n",
    "        print(f\"Results for {model_name} \\n\")\n",
    "\n",
    "        model_obj.fit(X_train,y_train)\n",
    "\n",
    "        y_pred_train_proba = model_obj.predict_proba(X_train)\n",
    "        y_pred_test_proba = model_obj.predict_proba(X_test)\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "\n",
    "            y_train_pred = (y_pred_train_proba[:,1]>threshold).astype(int)\n",
    "            y_test_pred = (y_pred_test_proba[:,1]>threshold).astype(int)\n",
    "\n",
    "            train_metrics = evaluate_clf(y_train,y_train_pred)\n",
    "            test_metrics = evaluate_clf(y_test,y_test_pred)\n",
    "\n",
    "            train_cost = total_cost(y_train,y_train_pred)\n",
    "            test_cost = total_cost(y_test,y_test_pred)\n",
    "\n",
    "            f1_score_train = round(train_metrics[1],4)\n",
    "            f1_score_test  = round(test_metrics[1],4)\n",
    "            roc_auc_train  = round(train_metrics[-1],4)\n",
    "            roc_auc_test   = round(test_metrics[-1],4)\n",
    "\n",
    "            cost_list.append(test_cost)\n",
    "            \n",
    "            print(f\"RESULTS FOR THRESHOLD: {threshold}\")\n",
    "            print(f\"TRAINING: F1-score: {f1_score_train}, ROC AUC: {roc_auc_train}, Cost: {train_cost}\")\n",
    "            print(f\"TESTING : F1-score: {f1_score_test}, ROC AUC: {roc_auc_test}, Cost: {test_cost}\")\n",
    "    \n",
    "            print(\"CONFUSION MATRICES:\\n\",confusion_matrix(y_train,y_train_pred), \"\\n\",confusion_matrix(y_test,y_test_pred))\n",
    "    \n",
    "    return sorted(cost_list,reverse=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res, y_train_res = handle_imbalance(X_train[highly_corr_cols],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models(X_train_res, X_test[highly_corr_cols], y_train_res, y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_score = model.feature_importances_*100\n",
    "features = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip(importance_score,features)),reverse=True)[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = [feat for score,feat in sorted(list(zip(importance_score,features)),reverse=True)[:100] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the number of folds (K)\n",
    "n_splits = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=11)\n",
    "X_test = X_test[highly_corr_cols]\n",
    "for test_index, val_index in skf.split(X_test,y_test):\n",
    "    #print(test_index,val_index)\n",
    "    X_test_cv, X_val = X_test.iloc[test_index], X_test.iloc[val_index]\n",
    "    #print(X_train_cv)\n",
    "    y_test_cv, y_val = y_test.iloc[test_index], y_test.iloc[val_index]\n",
    "\n",
    "    # Apply SMOTE to X_train_cv and y_train_cv\n",
    "    #X_train_res, y_train_res = handle_imbalance(X_train_cv, y_train_cv)\n",
    "\n",
    "    #model.fit(X_train_res,y_train_res)\n",
    "    \n",
    "    print(\"##################################\\n\")\n",
    "    for threshold in [i*0.001 for i in range(4,10)]:\n",
    "        \n",
    "        y_val_pred_proba = model.predict_proba(X_val)\n",
    "        y_val_pred = (y_val_pred_proba[:,1]>threshold).astype(int)\n",
    "\n",
    "        metrics = evaluate_clf(y_val,y_val_pred)\n",
    "        cost = total_cost(y_val, y_val_pred)\n",
    "        print(f\"Threshold {threshold}: F1-score: {round(metrics[1],4)}, ROC AUC: {round(metrics[-1],4)}, Cost: {cost}\")\n",
    "        print(confusion_matrix(y_val,y_val_pred))\n",
    "    print(\"##################################\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_splits = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for test_index, val_index in skf.split(X_test, y_test):\n",
    "    #print(train_index,val_index)\n",
    "    X_test_cv, X_val = X_test.iloc[test_index], X_test.iloc[val_index]\n",
    "    #print(X_train_cv)\n",
    "    y_test_cv, y_val = y_test[test_index], y_test[val_index]\n",
    "\n",
    "    \n",
    "\n",
    "    for threshold in thresholds:\n",
    "        \n",
    "        y_val_pred_proba = model.predict_proba(X_val)\n",
    "        y_val_pred = (y_val_pred_proba[:,1]>threshold).astype(int)\n",
    "\n",
    "        metrics = evaluate_clf(y_val,y_val_pred)\n",
    "        cost = total_cost(y_val, y_val_pred)\n",
    "        print(f\"Threshold {threshold}: F1-score: {round(metrics[1],4)}, ROC AUC: {round(metrics[-1],4)}, Cost: {cost*5}\")\n",
    "        print(confusion_matrix(y_val,y_val_pred)*5)\n",
    "    print(\"##################################\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT #1 \n",
    "\n",
    "- imputation: Simple Imputer with mean, median & constant values\n",
    "- imbalance handling: SMOTETomek\n",
    "- no outlier handling\n",
    "- scaling : Standard Scaler\n",
    "- no correlation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = restore_original_data()\n",
    "df_1 = drop_zero_std(df_1)\n",
    "df_1 = drop_duplicated_cols(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[highly_corr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train,y_train],axis=1)\n",
    "df_test  = pd.concat([X_test,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"first we need to split df_merged[highly_corr_cols] into df_train, df_test then \n",
    "    perform imputation, balancing and scaling on train dataset, train the model\n",
    "    then apply scale_ params (mean,std etc.) to test dataset to evaluate model performance\n",
    "    \"\"\"\n",
    "\n",
    "handle_missing = HandleMissingValues(df_train)\n",
    "\n",
    "methods= [\"mean\",\"median\",\"constant\",\"knn\"]\n",
    "\n",
    "imputed_df_dict = {\n",
    "    \"mean_imputed_df\":None,\n",
    "    \"median_imputed_df\":None,\n",
    "    \"constant_imputed_df\":None,\n",
    "    \"knn_imputed_df\":None\n",
    "}\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "    imputed_df_dict[f\"{method}_imputed_df\"] = handle_missing.handle_imputation(method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df_dict[\"mean_imputed_df\"].isna().sum().sum()+ imputed_df_dict[\"median_imputed_df\"].isna().sum().sum() + \\\n",
    "imputed_df_dict[\"constant_imputed_df\"].isna().sum().sum() + imputed_df_dict[\"knn_imputed_df\"].isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in imputed_df_dict.items():\n",
    "\n",
    "    \n",
    "    X_balanced, y_balanced = handle_imbalance(df.iloc[:,:-1],df.iloc[:,-1])\n",
    "    imputed_df_dict[name] = (X_balanced,y_balanced) # store as tuple\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df_dict[\"mean_imputed_df\"][1].value_counts()  # which is y_train of mean imputed df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imp_X_train,mean_imp_y_train = imputed_df_dict[\"mean_imputed_df\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_scaler= StandardScaler()\n",
    "mean_imp_X_train_scaled= s_scaler.fit_transform(mean_imp_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = s_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models(mean_imp_X_train_scaled,X_test_scaled,mean_imp_y_train,y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment#1 standard-scaler, mean imputation, balanced train dataset conclusions\n",
    "- for scale_pos_weight = 20, reg_alpha = 0.1:\n",
    "    - best threshold range : [0.28-0.35] : take 0.3 -> roc_auc = 0.655, f1-score = 0.2917, cost= 133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imp_X_train,median_imp_y_train = imputed_df_dict[\"median_imputed_df\"]\n",
    "s_scaler= StandardScaler()\n",
    "median_imp_X_train_scaled= s_scaler.fit_transform(median_imp_X_train)\n",
    "X_test_scaled = s_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models(median_imp_X_train_scaled,X_test_scaled,median_imp_y_train,y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_scaler = RobustScaler()\n",
    "median_imp_X_train_scaled= r_scaler.fit_transform(median_imp_X_train)\n",
    "X_test_scaled = r_scaler.transform(X_test)\n",
    "eval_models(median_imp_X_train_scaled,X_test_scaled,median_imp_y_train,y_test,models,thresholds)\n",
    "# cost 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment#2 standard-scaler, median imputation, balanced train dataset conclusions\n",
    "- for scale_pos_weight = 20, reg_alpha = 0.1:\n",
    "    - best threshold range : [0.35-0.3675] : take 0.36 -> F1-score: 0.3182, ROC AUC: 0.6617, cost= 129\n",
    "\n",
    "- if robust scaler is applied: cost = 127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_imp_X_train,constant_imp_y_train = imputed_df_dict[\"constant_imputed_df\"]\n",
    "s_scaler= StandardScaler()\n",
    "constant_imp_X_train_scaled= s_scaler.fit_transform(constant_imp_X_train)\n",
    "X_test_scaled = s_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models(constant_imp_X_train_scaled,X_test_scaled,constant_imp_y_train,y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_scaler= RobustScaler()\n",
    "constant_imp_X_train_scaled= r_scaler.fit_transform(constant_imp_X_train)\n",
    "X_test_scaled = r_scaler.transform(X_test)\n",
    "eval_models(constant_imp_X_train_scaled,X_test_scaled,constant_imp_y_train,y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment#3 standard-scaler, constant imputation, balanced train dataset conclusions\n",
    "- for scale_pos_weight = 30, reg_alpha = 0.1:\n",
    "    - best threshold range : [0.25-0.275] : take 0.26 -> F1-score: 0.3404, ROC AUC: 0.686, Cost: 121\n",
    "\n",
    "- if robust scaler is applied : cost = 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600,step=50),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, step=0.01),\n",
    "        \"scale_pos_weight\": trial.suggest_int(\"scale_pos_weight\", 5, 50, step=5),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1.0, step = 0.1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1.0,step = 0.1),\n",
    "    }\n",
    "\n",
    "    # Create and train the XGBoost model\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(constant_imp_X_train_scaled, constant_imp_y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
    " \n",
    "    threshold = trial.suggest_float(\"threshold\", 0.005, 0.4, step=0.005) # Add threshold as a hyperparameter\n",
    "    y_pred = (y_pred_proba[:, 1] > threshold).astype(int)\n",
    "\n",
    "    # Calculate the ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    cost = total_cost(y_test,y_pred)\n",
    "\n",
    "    # Return the negative ROC AUC as Optuna tries to minimize the objective\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")  # \"maximize\" if you want to maximize ROC AUC, minimze for cost metric\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_026 = best_params\n",
    "# 0.6913793103448276 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    " 'n_estimators': 550,\n",
    " 'max_depth': 5,\n",
    " 'learning_rate': 0.060000000000000005,\n",
    " 'scale_pos_weight': 50,\n",
    " 'reg_alpha': 0.2,\n",
    " 'reg_lambda': 0.6000000000000001\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(constant_imp_X_train_scaled, constant_imp_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "threshold = 0.13\n",
    "y_pred = (y_pred_proba[:, 1] > threshold).astype(int)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "cost = total_cost(y_test,y_pred)\n",
    "\n",
    "print(roc_auc, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imp_X_train,knn_imp_y_train = imputed_df_dict[\"knn_imputed_df\"]\n",
    "s_scaler= StandardScaler()\n",
    "knn_imp_X_train_scaled= s_scaler.fit_transform(knn_imp_X_train)\n",
    "X_test_scaled = s_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models(knn_imp_X_train_scaled,X_test_scaled,knn_imp_y_train,y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_scaler= RobustScaler()\n",
    "knn_imp_X_train_scaled= r_scaler.fit_transform(knn_imp_X_train)\n",
    "X_test_scaled = r_scaler.transform(X_test)\n",
    "eval_models(knn_imp_X_train_scaled,X_test_scaled,knn_imp_y_train,y_test,models,thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment#4 standard-scaler, knn imputation, balanced train dataset conclusions\n",
    "- for scale_pos_weight = 20, reg_alpha = 0.1:\n",
    "    - best threshold range : [0.093-0.1] : take 0.095 -> F1-score: 0.3182, ROC AUC: 0.6617, Cost: 129\n",
    "- if robust scaler is applied: cost = 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator , TransformerMixin\n",
    "from sklearn.pipeline import Pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we are better to apply transformation pipeline consisting of :\n",
    "- [drop_zero_std -> drop_duplicated_cols -> constant(0) imputation -> SMOTOMEK balancing -> Robust Scaling ] \n",
    "\n",
    "- then train model XGBClassifier using best params: {scale_pos_weight:30,reg_alpha:0.1} with threshold from the range: [0.25-0.275]\n",
    "  \n",
    "  to get the best results: F1-score: 0.3404, ROC AUC: 0.686, Cost: 121 -> Which yields 33% less costs than the initial problem case.\n",
    "  furthermore, i conducted hyperparameter optimization but there was no significant improvement. So i will keep this result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model =  XGBClassifier(\n",
    "        random_state=11,\n",
    "        scale_pos_weight=30,\n",
    "        reg_alpha=0.01),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"best_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_model_path,\"wb\") as f:\n",
    "    dill.dump(best_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"constant_imputer\",SimpleImputer(strategy=\"constant\",fill_value=0)),\n",
    "        (\"scaler\",RobustScaler())       \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPreprocessor(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,use_y = True):\n",
    "        self.use_y = use_y\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        if self.use_y:\n",
    "            self.y = y \n",
    "\n",
    "        return self \n",
    "    \n",
    "    def transform(self,X,is_testing=False):\n",
    "        X_transformed = drop_zero_std(X)\n",
    "        X_transformed = drop_duplicated_cols(X_transformed)\n",
    "        X_transformed = X_transformed[highly_corr_cols[:-1]]\n",
    "\n",
    "        imputer = SimpleImputer(strategy=\"constant\",fill_value=0)\n",
    "        X_transformed = imputer.fit_transform(X_transformed)\n",
    "\n",
    "        if not is_testing:\n",
    "            X_transformed , self.y = handle_imbalance(X_transformed,self.y)\n",
    "            \n",
    "        r_scaler = RobustScaler()\n",
    "        X_transformed = r_scaler.fit_transform(X_transformed)\n",
    "\n",
    "        return X_transformed, self.y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_obj = TrainingPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_obj.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"preprocessor_obj.pkl\"\n",
    "with open(file_path,\"wb\") as f :\n",
    "    dill.dump(preprocessor_obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed, y_res = preprocessor_obj.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ayhan\\Desktop\\predictive-wafer-maintenance\\notebook\\EDA.ipynb Cell 208\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayhan/Desktop/predictive-wafer-maintenance/notebook/EDA.ipynb#Y455sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
